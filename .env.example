# ─────────────────────────────────────────────────────────────────────────────
# AI Client Configuration
# Copy this file to .env and fill in the values for your provider.
# All variables have sensible defaults; only override what you need.
# ─────────────────────────────────────────────────────────────────────────────

# ── Local: LM Studio (default) ───────────────────────────────────────────────
AI_BASE_URL=http://localhost:1234/v1
AI_MODEL=google/gemma-3-12b
AI_API_KEY=lm-studio

# ── Local: Ollama ────────────────────────────────────────────────────────────
# AI_BASE_URL=http://localhost:11434/v1
# AI_MODEL=llama3
# AI_API_KEY=ollama

# ── Online: OpenAI ───────────────────────────────────────────────────────────
# AI_BASE_URL=https://api.openai.com/v1
# AI_MODEL=gpt-4o
# AI_API_KEY=sk-...

# ── Online: Google Gemini (OpenAI-compatible proxy) ──────────────────────────
# AI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
# AI_MODEL=gemini-2.0-flash
# AI_API_KEY=AIza...

# ── Tuning ───────────────────────────────────────────────────────────────────
AI_TIMEOUT=300          # seconds to wait for a response
AI_MAX_TOKENS=2048      # 0 = omit (use provider default)
AI_TEMPERATURE=0.2      # lower = more deterministic
CONTEXT_TURNS=4         # number of past conversation turns to keep
